{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T18:34:16.042401Z",
     "start_time": "2020-11-24T18:34:15.860986Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T18:34:16.738351Z",
     "start_time": "2020-11-24T18:34:16.043601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook\n",
    "import PIL\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from os.path import join\n",
    "import scipy.special\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import scipy\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from csv import DictWriter\n",
    "import nengolib\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "DoubleTensor = torch.cuda.DoubleTensor if use_cuda else torch.DoubleTensor\n",
    "\n",
    "IntTensor = torch.cuda.IntTensor if use_cuda else torch.IntTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "ttype = FloatTensor\n",
    "\n",
    "import seaborn as sns\n",
    "print(use_cuda)\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T18:34:16.741380Z",
     "start_time": "2020-11-24T18:34:16.739388Z"
    }
   },
   "outputs": [],
   "source": [
    "sn.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T18:34:16.754866Z",
     "start_time": "2020-11-24T18:34:16.742214Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class coRNNCell(nn.Module):\n",
    "    def __init__(self, n_inp, n_hid, dt, gamma, epsilon):\n",
    "        super(coRNNCell, self).__init__()\n",
    "        self.dt = dt\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.i2h = nn.Linear(n_inp + n_hid + n_hid, n_hid)\n",
    "\n",
    "    def forward(self,x,hy,hz):\n",
    "        hz = hz + self.dt * (torch.tanh(self.i2h(torch.cat((x, hz, hy),1)))\n",
    "                                   - self.gamma * hy - self.epsilon * hz)\n",
    "        hy = hy + self.dt * hz\n",
    "\n",
    "        return hy, hz\n",
    "\n",
    "class coRNN(nn.Module):\n",
    "    def __init__(self, n_inp, n_hid, n_out, dt, gamma, epsilon):\n",
    "        super(coRNN, self).__init__()\n",
    "        self.n_hid = n_hid\n",
    "        self.cell = coRNNCell(n_inp,n_hid,dt,gamma,epsilon)\n",
    "        self.readout = nn.Linear(n_hid, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        ## initialize hidden states\n",
    "        hy = Variable(torch.zeros(x.size(1),self.n_hid)).to(device)\n",
    "        hz = Variable(torch.zeros(x.size(1),self.n_hid)).to(device)\n",
    "\n",
    "        for t in range(x.size(0)):\n",
    "            hy, hz = self.cell(x[t],hy,hz)\n",
    "            outputs.append(hy)\n",
    "        outputs = torch.stack(outputs)\n",
    "        \n",
    "        output = self.readout(outputs)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T17:56:00.067020Z",
     "start_time": "2020-11-23T17:56:00.056891Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def mackey_glass(sample_len=1000, tau=17, delta_t=10, seed=None, n_samples=1):\n",
    "    # Adapted from https://github.com/mila-iqia/summerschool2015/blob/master/rnn_tutorial/synthetic.py\n",
    "    '''\n",
    "    mackey_glass(sample_len=1000, tau=17, seed = None, n_samples = 1) -> input\n",
    "    Generate the Mackey Glass time-series. Parameters are:\n",
    "        - sample_len: length of the time-series in timesteps. Default is 1000.\n",
    "        - tau: delay of the MG - system. Commonly used values are tau=17 (mild \n",
    "          chaos) and tau=30 (moderate chaos). Default is 17.\n",
    "        - seed: to seed the random generator, can be used to generate the same\n",
    "          timeseries at each invocation.\n",
    "        - n_samples : number of samples to generate\n",
    "    '''\n",
    "    history_len = tau * delta_t \n",
    "    # Initial conditions for the history of the system\n",
    "    timeseries = 1.2\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        history = collections.deque(1.2 * np.ones(history_len) + 0.2 * \\\n",
    "                                    (np.random.rand(history_len) - 0.5))\n",
    "        # Preallocate the array for the time-series\n",
    "        inp = np.zeros((sample_len,1))\n",
    "        \n",
    "        for timestep in range(sample_len):\n",
    "            for _ in range(delta_t):\n",
    "                xtau = history.popleft()\n",
    "                history.append(timeseries)\n",
    "                timeseries = history[-1] + (0.2 * xtau / (1.0 + xtau ** 10) - \\\n",
    "                             0.1 * history[-1]) / delta_t\n",
    "            inp[timestep] = timeseries\n",
    "        \n",
    "        # Squash timeseries through tanh\n",
    "        inp = np.tanh(inp - 1)\n",
    "        samples.append(inp)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def generate_data(n_batches, length, split=0.5, seed=0,\n",
    "                  predict_length=15, tau=17, washout=100, delta_t=1,\n",
    "                  center=True):\n",
    "    X = np.asarray(mackey_glass(\n",
    "        sample_len=length+predict_length+washout, tau=tau,\n",
    "        seed=seed, n_samples=n_batches))\n",
    "    X = X[:, washout:, :]\n",
    "    cutoff = int(split*n_batches)\n",
    "    if center:\n",
    "        X -= np.mean(X)  # global mean over all batches, approx -0.066\n",
    "    Y = X[:, predict_length:, :]\n",
    "    X = X[:, :-predict_length, :]\n",
    "    assert X.shape == Y.shape\n",
    "    return ((X[:cutoff], Y[:cutoff]),\n",
    "            (X[cutoff:], Y[cutoff:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T17:56:09.308282Z",
     "start_time": "2020-11-23T17:56:00.723134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 5000, 1) (64, 5000, 1) (64, 5000, 1)\n"
     ]
    }
   ],
   "source": [
    "(train_X, train_Y), (test_X, test_Y) = generate_data(128, 5000)\n",
    "dataset = torch.utils.data.TensorDataset(torch.Tensor(train_X).cuda(), torch.Tensor(train_Y).cuda())\n",
    "dataset = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "dataset_valid = torch.utils.data.TensorDataset(torch.Tensor(test_X).cuda(), torch.Tensor(test_Y).cuda())\n",
    "dataset_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=64, shuffle=False)\n",
    "\n",
    "print(train_X.shape, train_Y.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T17:56:09.323101Z",
     "start_time": "2020-11-23T17:56:09.309540Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, ttype, train_loader, test_loader, optimizer, loss_func, epoch, perf_file,\n",
    "          loss_buffer_size=800, batch_size=4, device='cuda',\n",
    "          prog_bar=None):\n",
    "    \n",
    "    assert(loss_buffer_size%batch_size==0)\n",
    "        \n",
    "    losses = []\n",
    "    last_test_perf = 0\n",
    "    best_test_perf = 1000\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data = data.to(device).transpose(1,0)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data).transpose(1,0)\n",
    "        loss = loss_func(out,\n",
    "                         target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        losses = losses[int(-loss_buffer_size/batch_size):]\n",
    "        \n",
    "        if ((batch_idx*batch_size)%loss_buffer_size == 0):\n",
    "            loss_track = {}\n",
    "            last_test_perf = test_model(model, 'cuda', test_loader, \n",
    "                                  )\n",
    "            loss_track['avg_loss'] = np.mean(losses)\n",
    "            loss_track['last_test'] = last_test_perf\n",
    "            loss_track['epoch'] = epoch\n",
    "            loss_track['batch_idx'] = batch_idx\n",
    "            with open(perf_file, 'a+') as fp:\n",
    "                csv_writer = DictWriter(fp, fieldnames=list(loss_track.keys()))\n",
    "                if fp.tell() == 0:\n",
    "                    csv_writer.writeheader()\n",
    "                csv_writer.writerow(loss_track)\n",
    "                fp.flush()\n",
    "            if best_test_perf > last_test_perf:\n",
    "                torch.save(model.state_dict(), perf_file[:-4]+\".pt\")\n",
    "                best_test_perf = last_test_perf\n",
    "        if not (prog_bar is None):\n",
    "            # Update progress_bar\n",
    "            s = \"{}:{} Loss: {:.4f},valid: {:.4f}\"\n",
    "            format_list = [e,batch_idx*batch_size, np.mean(losses), \n",
    "                           last_test_perf]         \n",
    "            s = s.format(*format_list)\n",
    "            prog_bar.set_description(s)\n",
    "                \n",
    "def test_model(model, device, test_loader):\n",
    "    # Test the Model\n",
    "    nrmsd = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            data = x.to(device).transpose(1,0)\n",
    "            target = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data).transpose(1,0)\n",
    "            nrmsd.append(nengolib.signal.nrmse(out.detach().cpu().numpy().flatten(), \n",
    "                                               target=target.detach().cpu().numpy().flatten()))\n",
    "\n",
    "    perf = np.array(nrmsd).mean()\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T03:19:48.684504Z",
     "start_time": "2020-11-19T03:19:48.678393Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T18:34:28.097743Z",
     "start_time": "2020-11-24T18:34:26.352564Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Weights: 33281\n",
      "coRNN(\n",
      "  (cell): coRNNCell(\n",
      "    (i2h): Linear(in_features=258, out_features=128, bias=True)\n",
      "  )\n",
      "  (readout): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cornn_params = dict(n_inp=2,\n",
    "                    n_hid=128, \n",
    "                    n_out=1,\n",
    "                    dt=1.6e-2,\n",
    "                    gamma=94.5,\n",
    "                    epsilon=9.5)\n",
    "model = coRNN(**cornn_params).cuda()\n",
    "\n",
    "tot_weights = 0\n",
    "for p in model.parameters():\n",
    "    tot_weights += p.numel()\n",
    "print(\"Total Weights:\", tot_weights)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T19:02:36.537275Z",
     "start_time": "2020-11-23T19:02:35.583369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 64, 1])\n",
      "torch.Size([64, 5000, 1])\n"
     ]
    }
   ],
   "source": [
    "inp, target = next(enumerate(dataset_valid))[1]\n",
    "print(model(inp.transpose(1,0)).shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T20:11:04.149396Z",
     "start_time": "2020-11-23T19:02:43.095334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66603d0be749449ba3474451d505f9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bcb9110c5b87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_buffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperf_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'perf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mackeyglass_coRNN_9.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m           prog_bar=progress_bar)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-78670b6bd369>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, ttype, train_loader, test_loader, optimizer, loss_func, epoch, perf_file, loss_buffer_size, batch_size, device, prog_bar)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         loss = loss_func(out,\n\u001b[1;32m     18\u001b[0m                          target)\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e6acb36de04b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-2)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "progress_bar = tqdm(range(int(epochs)), bar_format='{l_bar}{bar:5}{r_bar}{bar:-5b}')\n",
    "last_perf = 1000\n",
    "for e in progress_bar:\n",
    "    train(model, ttype, dataset, dataset_valid, \n",
    "          optimizer, loss_func, batch_size=batch_size, loss_buffer_size=64,\n",
    "          epoch=e, perf_file=join('perf','mackeyglass_coRNN_9.csv'),\n",
    "          prog_bar=progress_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T03:18:55.614301Z",
     "start_time": "2020-11-19T03:18:21.205774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-12-b159598f1528>\u001b[0m(9)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      5 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      6 \u001b[0;31m    last_perf = train(model, ttype, train_X, train_Y, test_X, test_Y, \n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m                      \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_perf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_perf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m                      \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperf_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'perf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mackeyglass_LSTM_3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 9 \u001b[0;31m                      prog_bar=progress_bar, loss_buffer_size=32)\n",
      "\u001b[0m\n",
      "--KeyboardInterrupt--\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.279Z"
    }
   },
   "outputs": [],
   "source": [
    "dat = pd.read_csv(join('perf', 'mackeyglass_deepsith_.csv'))\n",
    "maxpres = 64\n",
    "dat['presnum_epoch'] = ((dat.batch_idx) + maxpres*dat.epoch)/maxpres\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "sn.lineplot(data=dat, x=dat.presnum_epoch, y='last_testperf', \n",
    "            color='blue', ax=ax)\n",
    "\n",
    "ax.grid(True)\n",
    "ax.set_title(dat.last_testperf.min())\n",
    "ax.legend([\"Training\", \n",
    "           ])\n",
    "ax.set_xlim(0, 500)\n",
    "ax.set_ylim(.00, .05)\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "sn.lineplot(data=dat, x=dat.presnum_epoch, y='avg_loss', \n",
    "            color='blue',ax=ax)\n",
    "\n",
    "ax.set_xlim(0, 500)\n",
    "ax.set_ylim(.00, .0001)\n",
    "ax.legend([\"Test NRMSD\"])\n",
    "ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.280Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.282Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('LMU_data.dil', 'rb') as f:\n",
    "    dats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.283Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7))\n",
    "\n",
    "plt.plot(np.arange(0, 500, 1), dats[0], linewidth=3)\n",
    "plt.plot(np.arange(0, 500, 1), dats[1], linewidth=3)\n",
    "plt.plot(np.arange(0, 500, 1), dats[2], linewidth=3)\n",
    "plt.plot(np.arange(0, 500, 1), perf['loss'][::32], color='purple', linewidth=3)\n",
    "plt.yscale('log')\n",
    "plt.legend(['LSTM', 'LMU', 'Hybrid', 'DeepSITH'])\n",
    "plt.savefig('performance')\n",
    "sns.despine(offset=15)\n",
    "plt.savefig('mackey_glass_perf', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Impulse-based SITH class\n",
    "class iSITH():\n",
    "    def __init__(self, tau_min=.1, tau_max=20., buff_max=100, k=8, ntau=10, dt=.01, g=0.0):\n",
    "        # save inputs\n",
    "        self.k = k\n",
    "        self.tau_min = tau_min\n",
    "        self.tau_max = tau_max\n",
    "        self.ntau = ntau\n",
    "        self.dt = dt\n",
    "        self.g = g        # calc tau_star\n",
    "        self.c = (self.tau_max/self.tau_min)**(1./(ntau-1))-1\n",
    "        self.tau_star = self.tau_min * (1 + self.c)**np.arange(ntau)        # determine the time points in steps of dt\n",
    "        self.times = np.arange(dt, buff_max+dt, dt)        # fill the ftilde filters\n",
    "        a = np.log(k)*k\n",
    "        b = np.log(np.arange(2,k)).sum()\n",
    "        A = ((1/self.tau_star)*(np.exp(a-b))*(self.tau_star**self.g))[:, np.newaxis]\n",
    "        self.filters = A*np.exp((np.log(self.times[np.newaxis, :]/self.tau_star[:, np.newaxis])*(k+1)) + \\\n",
    "                                (k*(-self.times[np.newaxis, :]/self.tau_star[:, np.newaxis])))        #A = ((1/self.tau_star)*(k**(k)/factorial(k-1))*(self.tau_star**self.g))[:, np.newaxis]\n",
    "        #self.filters = A*((self.times[np.newaxis, :]/self.tau_star[:, np.newaxis])**(k+1)) * \\\n",
    "        #                np.exp(k*(-self.times[np.newaxis, :]/self.tau_star[:, np.newaxis]))    def present(self, x):\n",
    "        # make sure input is features by time\n",
    "        x = np.atleast_2d(x)        # get num points to save from the convolution\n",
    "        xlen = x.shape[-1]        # loop over features and taustars performing the convolution\n",
    "        # note we're scaling the output by both dt and the k/(k+1)\n",
    "        return np.array([[signal.convolve(self.filters[i], x[j])[:xlen]\n",
    "                            for i in range(len(self.filters))]\n",
    "                         for j in range(len(x))])*self.dt*self.k/(self.k+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.286Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "idx = 9\n",
    "out = model(test_X[idx].view(1,1,1,-1))[0]\n",
    "plt.plot(test_Y[idx][-1000:].detach().cpu().numpy(), linewidth=5)\n",
    "plt.plot(out[-1000:].detach().cpu().numpy())\n",
    "plt.legend(['Test Out', 'DS Prediction'])\n",
    "model.train();\n",
    "print('Normalized RMSD:', np.sqrt(((test_Y[idx].detach().cpu().numpy() - out.detach().cpu().numpy())**2).mean()/\n",
    "                                  (test_Y[idx].detach().cpu().numpy()**2).mean()))\n",
    "sns.despine(offset=15)\n",
    "\n",
    "plt.savefig('long_boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.288Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out = model(train_X[idx].view(1,1,1,-1))[0]\n",
    "plt.plot(train_Y[idx][-1000:].detach().cpu().numpy())\n",
    "plt.plot(out[-1000:].detach().cpu().numpy())\n",
    "model.train();\n",
    "print('Normalized RMSD:', np.sqrt(((train_Y[idx].detach().cpu().numpy() - out.detach().cpu().numpy())**2).mean()/\n",
    "                                  (train_Y[idx].detach().cpu().numpy()**2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.290Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "nrmsd = []\n",
    "for idx in range(test_X.shape[0]):\n",
    "    out = model(test_X[idx].view(1,1,1,-1))[0]\n",
    "    nrmsd.append(np.sqrt(((test_Y[idx].detach().cpu().numpy() - out.detach().cpu().numpy())**2).mean()/\n",
    "                                  (test_Y[idx].detach().cpu().numpy()**2).mean()))\n",
    "model.train();\n",
    "i\n",
    "print('Avg Normalized RMSD:', np.array(nrmsd).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.291Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "def_scale = 5000\n",
    "res = {'nrmsd':[],\n",
    "       'sig_leng':[]}\n",
    "for num in range(1000, 10000, 500):\n",
    "    nrmsd = []\n",
    "    #_, (test_X, test_Y) = generate_data(128, num)\n",
    "    #train_X = ttype(train_X).view(64, 1, 1, num)\n",
    "    #train_Y = ttype(train_Y).view(64, num, 1)\n",
    "    #test_Xhalf = ttype(test_X).view(64, 1, 1, num)\n",
    "    #test_Yhalf = ttype(test_Y).view(64, num, 1)\n",
    "    for idx in range(test_X.shape[0]):\n",
    "        f = scipy.interpolate.interp1d(np.arange(0, def_scale, 1),\n",
    "                                       test_X[idx].view(-1).detach().cpu())\n",
    "        new_X = f(np.arange(0, def_scale-1*(num/def_scale), 1/(num/def_scale)))\n",
    "        \n",
    "        f = scipy.interpolate.interp1d(np.arange(0, def_scale, 1),\n",
    "                                       test_Y[idx].view(-1).detach().cpu())\n",
    "        new_Y = f(np.arange(0, def_scale-1*(num/def_scale), 1/(num/def_scale)))\n",
    "        \n",
    "        out = model(ttype(new_X).view(1,1,1,-1))[0]\n",
    "        nrmsd.append(np.sqrt(((new_Y - out.view(-1).detach().cpu().numpy())**2).mean()/\n",
    "                                      (new_Y**2).mean()))\n",
    "    res['nrmsd'].append(np.array(nrmsd).mean())\n",
    "    res['sig_leng'].append(num)\n",
    "    print('Avg Normalized RMSD at {}:'.format(num), np.array(nrmsd).mean())\n",
    "model.train();\n",
    "dat = pd.DataFrame(res)\n",
    "dat.to_pickle(\"DS_RES_MG.dill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.293Z"
    }
   },
   "outputs": [],
   "source": [
    "dat_2 = pd.read_pickle(open(\"LOLA_RES_MG.dill\", 'rb'))\n",
    "dat = pd.read_pickle(open(\"DS_RES_MG.dill\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-14T15:27:40.651185Z",
     "start_time": "2020-09-14T15:27:40.642947Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-17T15:01:57.295Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.plot(np.arange(1000, 9600, 500)/5000, dat['nrmsd'])\n",
    "plt.plot(np.arange(1000, 9600, 500)/5000, dat_2['nrmsd'])\n",
    "plt.legend(['DeepSITH', 'LoLa'])\n",
    "plt.xlabel(\"scale\")\n",
    "plt.ylabel(\"NRMSD\")\n",
    "plt.savefig(\"LoLavsDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
