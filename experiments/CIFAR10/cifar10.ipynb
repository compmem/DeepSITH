{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import os \n",
    "from os.path import join\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "sn.set_context(\"poster\")\n",
    "import itertools\n",
    "from csv import DictWriter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "ttype = torch.cuda.DoubleTensor if torch.cuda.is_available() else torch.DoubleTensor\n",
    "ctype = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "from deepsith import DeepSITH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "batch_size = 8\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ttype, train_loader, optimizer, loss_func, epoch, perf_file,\n",
    "          loss_buffer_size=200, batch_size=4, device='cuda', reg_loss=None,\n",
    "          prog_bar=None):\n",
    "\n",
    "    assert(loss_buffer_size%batch_size==0)\n",
    "    losses = []\n",
    "    perfs = []\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device).view(data.shape[0],1,data.shape[1],-1).type(ttype)\n",
    "        target = target.to(device).type(ctype)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_func(out[:, -1, :],\n",
    "                         target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        perfs.append((torch.argmax(out[:, -1, :], dim=-1) == \n",
    "                      target).sum().item())\n",
    "        perfs = perfs[int(-loss_buffer_size/batch_size):]\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        losses = losses[-loss_buffer_size:]\n",
    "        if not (prog_bar is None):\n",
    "            # Update progress_bar\n",
    "            s = \"{}:{} Loss: {:.6f}, perf: {:.6f}\"\n",
    "            format_list = [e,batch_idx, np.mean(losses), \n",
    "                           np.sum(perfs)/((len(perfs))*batch_size)]         \n",
    "            s = s.format(*format_list)\n",
    "            prog_bar.set_description(s)\n",
    "        if (batch_idx*batch_size)%loss_buffer_size == 1:\n",
    "            loss_track = {}\n",
    "            loss_track['avg_loss'] = np.mean(losses)\n",
    "            loss_track['epoch'] = epoch\n",
    "            loss_track['batch_idx'] = batch_idx\n",
    "            loss_track['train_perf']= np.sum(perfs)/((len(perfs))*batch_size)\n",
    "            with open(perf_file, 'a+') as fp:\n",
    "                csv_writer = DictWriter(fp, fieldnames=list(loss_track.keys()))\n",
    "                csv_writer.writerow(loss_track)\n",
    "                fp.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "def test(model, device, test_loader, scale, stop_early=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            f = scipy.interpolate.interp1d(np.arange(0, data.view(-1).shape[-1], 1),\n",
    "                                           data.view(-1).detach().cpu())\n",
    "            new_y = f(np.arange(0, data.view(-1).shape[-1]-1, 1/scale))\n",
    "            new_y = ttype(new_y).view(1, 1, 1, -1)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            out = model(new_y)\n",
    "            pred = out[:, -1].argmax(dim=-1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            count += 1\n",
    "            if count %10000 ==0:\n",
    "                print(scale, correct/count)\n",
    "            if stop_early:\n",
    "                if count > stop_early:\n",
    "                    break\n",
    "    return correct / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSITH_Classifier(nn.Module):\n",
    "    def __init__(self, out_features, layer_params, dropout=.5):\n",
    "        super(DeepSITH_Classifier, self).__init__()\n",
    "        last_hidden = layer_params[-1]['hidden_size']\n",
    "        self.hs = DeepSITH(layer_params=layer_params, dropout=dropout)\n",
    "        self.to_out = nn.Linear(last_hidden, out_features)\n",
    "    def forward(self, inp):\n",
    "        x = self.hs(inp)\n",
    "        x = self.to_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSITH_Classifier(\n",
      "  (hs): DeepSITH(\n",
      "    (layers): ModuleList(\n",
      "      (0): _DeepSITH_core(\n",
      "        (sith): iSITH(ntau=20, tau_min=1, tau_max=256.0, buff_max=768.0, dt=1, k=75, g=0.0)\n",
      "        (linear): Sequential(\n",
      "          (0): Linear(in_features=60, out_features=10, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): _DeepSITH_core(\n",
      "        (sith): iSITH(ntau=20, tau_min=1, tau_max=512.0, buff_max=1536.0, dt=1, k=75, g=0.0)\n",
      "        (linear): Sequential(\n",
      "          (0): Linear(in_features=200, out_features=20, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): _DeepSITH_core(\n",
      "        (sith): iSITH(ntau=20, tau_min=1, tau_max=1024.0, buff_max=3072.0, dt=1, k=75, g=0.0)\n",
      "        (linear): Sequential(\n",
      "          (0): Linear(in_features=400, out_features=30, bias=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropouts): ModuleList(\n",
      "      (0): Dropout(p=0.0, inplace=False)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (to_out): Linear(in_features=30, out_features=10, bias=True)\n",
      ")\n",
      "Layer 0 tensor([  1.0000,   1.3389,   1.7927,   2.4002,   3.2136,   4.3028,   5.7610,\n",
      "          7.7134,  10.3275,  13.8276,  18.5138,  24.7882,  33.1890,  44.4368,\n",
      "         59.4967,  79.6603, 106.6575, 142.8042, 191.2011, 256.0000],\n",
      "       dtype=torch.float64)\n",
      "Layer 1 tensor([  1.0000,   1.3887,   1.9284,   2.6778,   3.7185,   5.1638,   7.1707,\n",
      "          9.9575,  13.8276,  19.2016,  26.6644,  37.0275,  51.4183,  71.4021,\n",
      "         99.1526, 137.6884, 191.2011, 265.5117, 368.7031, 512.0000],\n",
      "       dtype=torch.float64)\n",
      "Layer 2 tensor([1.0000e+00, 1.4402e+00, 2.0743e+00, 2.9875e+00, 4.3028e+00, 6.1970e+00,\n",
      "        8.9253e+00, 1.2855e+01, 1.8514e+01, 2.6664e+01, 3.8403e+01, 5.5310e+01,\n",
      "        7.9660e+01, 1.1473e+02, 1.6524e+02, 2.3799e+02, 3.4276e+02, 4.9366e+02,\n",
      "        7.1099e+02, 1.0240e+03], dtype=torch.float64)\n",
      "Total Weights: 16970\n"
     ]
    }
   ],
   "source": [
    "sith_params1 = {\"in_features\":3, \n",
    "                \"tau_min\":1, \"tau_max\":256.0, \n",
    "                \"k\":75, 'dt':1,\n",
    "                \"ntau\":20, 'g':0.,  \n",
    "                \"ttype\":ttype, \n",
    "                \"hidden_size\":10, \"act_func\":nn.ReLU()\n",
    "               }\n",
    "sith_params2 = {\"in_features\":sith_params1['hidden_size'], \n",
    "                \"tau_min\":1, \"tau_max\":512.0, \n",
    "                \"k\":75, 'dt':1,\n",
    "                \"ntau\":20, 'g':0., \n",
    "                \"ttype\":ttype, \n",
    "                \"hidden_size\":20, \"act_func\":nn.ReLU()\n",
    "                }\n",
    "sith_params3 = {\"in_features\":sith_params2['hidden_size'], \n",
    "                \"tau_min\":1, \"tau_max\":1024.0, \n",
    "                \"k\":75, 'dt':1,\n",
    "                \"ntau\":20, 'g':0., \n",
    "                \"ttype\":ttype, \n",
    "                \"hidden_size\":30, \"act_func\":nn.ReLU()\n",
    "                }\n",
    "layer_params = [sith_params1, sith_params2, sith_params3]\n",
    "model = DeepSITH_Classifier(out_features=len(classes),\n",
    "                            layer_params=layer_params, \n",
    "                            dropout=.0).cuda().double()\n",
    "print(model)\n",
    "for i, l in enumerate(model.hs.layers):\n",
    "    print(\"Layer {}\".format(i), l.sith.tau_star)\n",
    "tot_weights = 0\n",
    "for p in model.parameters():\n",
    "    tot_weights += p.numel()\n",
    "print(\"Total Weights:\", tot_weights)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fdfbb6671340ffa30888a867f0f7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-1a13495284cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     train(model, ttype, trainloader, optimizer, loss_func, batch_size=batch_size,\n\u001b[0m\u001b[0;32m      8\u001b[0m           \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperf_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'perf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cifar10_deepsith_1.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m           prog_bar=progress_bar)\n",
      "\u001b[1;32m<ipython-input-34-f63aa50bf0d5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, ttype, train_loader, optimizer, loss_func, epoch, perf_file, loss_buffer_size, batch_size, device, reg_loss, prog_bar)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         perfs.append((torch.argmax(out[:, -1, :], dim=-1) == \n\u001b[0m\u001b[0;32m     21\u001b[0m                       target).sum().item())\n\u001b[0;32m     22\u001b[0m         \u001b[0mperfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mloss_buffer_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Just for visualizing average loss through time. \n",
    "loss_buffer_size = 50*8\n",
    "epochs = 40\n",
    "progress_bar = tqdm(range(int(epochs)), bar_format='{l_bar}{bar:5}{r_bar}{bar:-5b}')\n",
    "\n",
    "for e in progress_bar:\n",
    "    train(model, ttype, trainloader, optimizer, loss_func, batch_size=batch_size,\n",
    "          epoch=e, perf_file=join('perf','cifar10_deepsith_1.csv'),\n",
    "          prog_bar=progress_bar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
