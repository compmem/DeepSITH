{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:57:39.184101Z",
     "start_time": "2020-12-03T16:57:38.997252Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T18:29:47.565802Z",
     "start_time": "2020-12-05T18:29:47.560378Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from os.path import join\n",
    "\n",
    "from deepsith import DeepSITH\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "from csv import DictWriter\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "\n",
    "ttype =FloatTensor\n",
    "\n",
    "import seaborn as sn\n",
    "print(use_cuda)\n",
    "import pickle\n",
    "\n",
    "sn.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:57:40.061632Z",
     "start_time": "2020-12-03T16:57:40.058700Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeepSITH_Classifier(nn.Module):\n",
    "    def __init__(self, out_features, layer_params, dropout=.1):\n",
    "        super(DeepSITH_Classifier, self).__init__()\n",
    "        last_hidden = layer_params[-1]['hidden_size']\n",
    "        self.hs = DeepSITH(layer_params=layer_params, dropout=dropout)\n",
    "        self.to_out = nn.Linear(last_hidden, out_features)\n",
    "    def forward(self, inp):\n",
    "        x = self.hs(inp)\n",
    "        x = self.to_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:57:41.468289Z",
     "start_time": "2020-12-03T16:57:41.465864Z"
    }
   },
   "outputs": [],
   "source": [
    "# Same seed and supposed Permutation as the coRNN paper\n",
    "torch.manual_seed(12008)\n",
    "permute = torch.randperm(784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:57:42.535384Z",
     "start_time": "2020-12-03T16:57:42.527542Z"
    }
   },
   "outputs": [],
   "source": [
    "print(permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:57:45.892964Z",
     "start_time": "2020-12-03T16:57:45.890797Z"
    }
   },
   "outputs": [],
   "source": [
    "norm = transforms.Normalize((.1307,), (.3081,), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:57:46.269650Z",
     "start_time": "2020-12-03T16:57:46.255928Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((.1307,), (.3081,))\n",
    "                               ])\n",
    "ds1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "ds2 = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "train_loader=torch.utils.data.DataLoader(ds1,batch_size=batch_size, \n",
    "                                         num_workers=1, pin_memory=True, shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(ds2, batch_size=batch_size, \n",
    "                                        num_workers=1, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:57:51.113709Z",
     "start_time": "2020-12-03T16:57:47.283835Z"
    }
   },
   "outputs": [],
   "source": [
    "testi = next(iter(test_loader))[0]\n",
    "\n",
    "plt.imshow(testi[0].reshape(-1)[permute].reshape(28,28))\n",
    "\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T16:57:51.164838Z",
     "start_time": "2020-12-03T16:57:51.162192Z"
    }
   },
   "outputs": [],
   "source": [
    "testi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T15:09:47.998604Z",
     "start_time": "2020-12-04T15:09:47.988228Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, ttype, train_loader, test_loader, optimizer, loss_func, epoch, perf_file,\n",
    "          permute=None, loss_buffer_size=800, batch_size=4, device='cuda',\n",
    "          prog_bar=None):\n",
    "    \n",
    "    assert(loss_buffer_size%batch_size==0)\n",
    "    if permute is None:\n",
    "        permute = torch.LongTensor(list(range(784)))\n",
    "        \n",
    "    losses = []\n",
    "    perfs = []\n",
    "    last_test_perf = 0\n",
    "    best_test_perf = -1\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data = data.to(device).view(data.shape[0],1,1,-1)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data[:, :, :, permute])\n",
    "        loss = loss_func(out[:, -1, :],\n",
    "                         target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        perfs.append((torch.argmax(out[:, -1, :], dim=-1) == \n",
    "                      target).sum().item())\n",
    "        perfs = perfs[int(-loss_buffer_size/batch_size):]\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        losses = losses[int(-loss_buffer_size/batch_size):]\n",
    "        if not (prog_bar is None):\n",
    "            # Update progress_bar\n",
    "            s = \"{}:{} Loss: {:.4f}, perf: {:.4f}, valid: {:.4f}\"\n",
    "            format_list = [e,batch_idx*batch_size, np.mean(losses), \n",
    "                           np.sum(perfs)/((len(perfs))*batch_size), last_test_perf]         \n",
    "            s = s.format(*format_list)\n",
    "            prog_bar.set_description(s)\n",
    "        \n",
    "        if ((batch_idx*batch_size)%loss_buffer_size == 0) & (batch_idx != 0):\n",
    "            loss_track = {}\n",
    "            # last_test_perf = test(model, 'cuda', test_loader, \n",
    "            #                       batch_size=batch_size, \n",
    "            #                       permute=permute)\n",
    "            loss_track['avg_loss'] = np.mean(losses)\n",
    "            #loss_track['last_test'] = last_test_perf\n",
    "            loss_track['epoch'] = epoch\n",
    "            loss_track['batch_idx'] = batch_idx\n",
    "            loss_track['train_perf']= np.sum(perfs)/((len(perfs))*batch_size)\n",
    "            with open(perf_file, 'a+') as fp:\n",
    "                csv_writer = DictWriter(fp, fieldnames=list(loss_track.keys()))\n",
    "                if fp.tell() == 0:\n",
    "                    csv_writer.writeheader()\n",
    "                csv_writer.writerow(loss_track)\n",
    "                fp.flush()\n",
    "            #if best_test_perf < last_test_perf:\n",
    "            #    torch.save(model.state_dict(), perf_file[:-4]+\".pt\")\n",
    "            #    best_test_perf = last_test_perf\n",
    "\n",
    "                \n",
    "def test(model, device, test_loader, batch_size=4, permute=None):\n",
    "    if permute is None:\n",
    "        permute = torch.LongTensor(list(range(784)))\n",
    "        \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device).view(data.shape[0],1,1,-1)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            out = model(data[:,:,:, permute])\n",
    "            pred = out[:, -1].argmax(dim=-1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            count += 1\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T15:09:48.936759Z",
     "start_time": "2020-12-04T15:09:48.926780Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = 0.0\n",
    "sith_params1 = {\"in_features\":1, \n",
    "                \"tau_min\":1, \"tau_max\":30.0, \"buff_max\":50,\n",
    "                \"k\":125, 'dt':1,\n",
    "                \"ntau\":20, 'g':g,  \n",
    "                \"ttype\":ttype, \"batch_norm\":True,\n",
    "                \"hidden_size\":60, \"act_func\":nn.ReLU()\n",
    "               }\n",
    "sith_params2 = {\"in_features\":sith_params1['hidden_size'], \n",
    "                \"tau_min\":1, \"tau_max\":150.0, \"buff_max\":250,\n",
    "                \"k\":61, 'dt':1,\n",
    "                \"ntau\":20, 'g':g, \n",
    "                \"ttype\":ttype, \"batch_norm\":True,\n",
    "                \"hidden_size\":60, \"act_func\":nn.ReLU()\n",
    "                }\n",
    "sith_params3 = {\"in_features\":sith_params2['hidden_size'], \n",
    "                \"tau_min\":1, \"tau_max\":750.0, \"buff_max\":1500,\n",
    "                \"k\":35, 'dt':1,\n",
    "                \"ntau\":20, 'g':g, \n",
    "                \"ttype\":ttype, \"batch_norm\":True,\n",
    "                \"hidden_size\":60, \"act_func\":nn.ReLU()\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "layer_params = [sith_params1, sith_params2, sith_params3]\n",
    "\n",
    "\n",
    "model = DeepSITH_Classifier(10,\n",
    "                           layer_params=layer_params, \n",
    "                           dropout=0.2).cuda()\n",
    "\n",
    "tot_weights = 0\n",
    "for p in model.parameters():\n",
    "    tot_weights += p.numel()\n",
    "print(\"Total Weights:\", tot_weights)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T00:17:49.489853Z",
     "start_time": "2020-12-04T15:09:54.978362Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 90\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "sched = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "progress_bar = tqdm(range(int(epochs)), bar_format='{l_bar}{bar:5}{r_bar}{bar:-5b}')\n",
    "test_perf = []\n",
    "for e in progress_bar:\n",
    "    train(model, ttype, train_loader, test_loader, optimizer, loss_func, batch_size=batch_size,\n",
    "          epoch=e, perf_file=join('perf','pmnist_deepsith_78.csv'),loss_buffer_size=64*32, \n",
    "          prog_bar=progress_bar, permute=permute)\n",
    "    last_test_perf = test(model, 'cuda', test_loader, \n",
    "                                  batch_size=batch_size, \n",
    "                                  permute=permute)\n",
    "    test_perf.append({\"epoch\":e,\n",
    "                      'test':last_test_perf})\n",
    "    sched.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-02T01:31:22.277851Z",
     "start_time": "2020-11-02T01:31:22.275272Z"
    }
   },
   "source": [
    "# Find Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T00:26:42.879054Z",
     "start_time": "2020-12-05T00:26:29.019237Z"
    }
   },
   "outputs": [],
   "source": [
    "test(model, 'cuda', test_loader, \n",
    "     batch_size=batch_size, \n",
    "     permute=permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T14:51:23.948941Z",
     "start_time": "2020-11-10T14:51:23.944246Z"
    }
   },
   "outputs": [],
   "source": [
    "def conf_mat_gen(model, device, test_loader, batch_size=4, permute=None):\n",
    "    if permute is None:\n",
    "        permute = torch.LongTensor(list(range(784)))\n",
    "    evals = {'pred':[],\n",
    "             'actual':[]}\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device).view(data.shape[0],1,1,-1)\n",
    "            target = target.to(device)\n",
    "            for x in target:\n",
    "                evals['actual'].append(x.detach().cpu().numpy())\n",
    "            out = model(data[:,:,:, permute])\n",
    "            for x in out[:, -1].argmax(dim=-1, keepdim=True):\n",
    "                evals['pred'].append(x.detach().cpu().numpy())\n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T14:51:36.918179Z",
     "start_time": "2020-11-10T14:51:25.179622Z"
    }
   },
   "outputs": [],
   "source": [
    "evals = conf_mat_gen(model, 'cuda', test_loader, batch_size=4, permute=permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T14:53:19.261434Z",
     "start_time": "2020-11-10T14:53:18.926905Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,11))\n",
    "plt.imshow(confusion_matrix(np.array(evals['pred'])[:, 0], \n",
    "                            np.array(evals['actual'])), cmap='coolwarm')\n",
    "plt.colorbar()\n",
    "plt.xticks(list(range(10)));\n",
    "plt.yticks(list(range(10)));\n",
    "plt.savefig(join('figs', 'sMNIST_LoLa_conf'), dpi=200, bboxinches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
