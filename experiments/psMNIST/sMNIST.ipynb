{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T13:46:07.365933Z",
     "start_time": "2021-03-15T13:46:07.144247Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T13:46:08.069258Z",
     "start_time": "2021-03-15T13:46:07.366877Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from os.path import join\n",
    "\n",
    "from deepsith import DeepSITH\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "from csv import DictWriter\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "\n",
    "ttype =FloatTensor\n",
    "\n",
    "import seaborn as sn\n",
    "print(use_cuda)\n",
    "import pickle\n",
    "\n",
    "sn.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T13:46:08.073469Z",
     "start_time": "2021-03-15T13:46:08.070605Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeepSITH_Classifier(nn.Module):\n",
    "    def __init__(self, out_features, layer_params, dropout=.1):\n",
    "        super(DeepSITH_Classifier, self).__init__()\n",
    "        last_hidden = layer_params[-1]['hidden_size']\n",
    "        self.hs = DeepSITH(layer_params=layer_params, dropout=dropout)\n",
    "        self.to_out = nn.Linear(last_hidden, out_features)\n",
    "    def forward(self, inp):\n",
    "        x = self.hs(inp)\n",
    "        x = self.to_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T13:46:08.083443Z",
     "start_time": "2021-03-15T13:46:08.074275Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "from deepsith import iSITH\n",
    "def min_fun(x, *args):\n",
    "    ntau = int(x[0])\n",
    "    k = int(x[1])\n",
    "    if k < 4 or k>125:\n",
    "        return np.inf\n",
    "    tau_min = args[0]\n",
    "    tau_max = args[1]    \n",
    "    ev = iSITH(tau_min=tau_min, tau_max=tau_max, buff_max=tau_max*5, k=k, ntau=ntau, dt=1, g=1.0)    \n",
    "    std_0 = ev.filters[:, 0, 0, :].detach().cpu().T.numpy()[::-1].sum(1)[int(tau_min):int(tau_max)].std()\n",
    "    std_1 = ev.filters[:, 0, 0, :].detach().cpu().T.numpy()[::-1, ::2].sum(1)[int(tau_min):int(tau_max)].std()    \n",
    "    to_min = std_0/std_1\n",
    "    return to_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T13:46:36.407379Z",
     "start_time": "2021-03-15T13:46:36.405404Z"
    }
   },
   "outputs": [],
   "source": [
    "norm = transforms.Normalize((.1307,), (.3081,), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T13:46:37.836490Z",
     "start_time": "2021-03-15T13:46:37.822290Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((.1307,), (.3081,))\n",
    "                               ])\n",
    "ds1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "ds2 = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "train_loader=torch.utils.data.DataLoader(ds1,batch_size=batch_size, \n",
    "                                         num_workers=1, pin_memory=True, shuffle=True)\n",
    "test_loader=torch.utils.data.DataLoader(ds2, batch_size=batch_size, \n",
    "                                        num_workers=1, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T13:44:18.485182Z",
     "start_time": "2021-03-15T13:44:14.672022Z"
    }
   },
   "outputs": [],
   "source": [
    "test = next(iter(test_loader))[0]\n",
    "\n",
    "plt.imshow(test[0].reshape(-1).reshape(28,28))\n",
    "\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T13:46:11.244757Z",
     "start_time": "2021-03-15T13:46:11.235002Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, ttype, train_loader, test_loader, optimizer, loss_func, epoch, perf_file,\n",
    "          loss_buffer_size=800, batch_size=4, device='cuda',\n",
    "          prog_bar=None, last_test_perf=0):\n",
    "    \n",
    "    assert(loss_buffer_size%batch_size==0)\n",
    "\n",
    "        \n",
    "    losses = []\n",
    "    perfs = []\n",
    "    best_test_perf = -1\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data = data.to(device).view(data.shape[0],1,1,-1)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_func(out[:, -1, :],\n",
    "                         target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        perfs.append((torch.argmax(out[:, -1, :], dim=-1) == \n",
    "                      target).sum().item())\n",
    "        perfs = perfs[int(-loss_buffer_size/batch_size):]\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        losses = losses[int(-loss_buffer_size/batch_size):]\n",
    "        if not (prog_bar is None):\n",
    "            # Update progress_bar\n",
    "            s = \"{}:{} Loss: {:.4f}, perf: {:.4f}, valid: {:.4f}\"\n",
    "            format_list = [e,batch_idx*batch_size, np.mean(losses), \n",
    "                           np.sum(perfs)/((len(perfs))*batch_size), last_test_perf]         \n",
    "            s = s.format(*format_list)\n",
    "            prog_bar.set_description(s)\n",
    "        \n",
    "        if ((batch_idx*batch_size)%loss_buffer_size == 0) & (batch_idx != 0):\n",
    "            loss_track = {}\n",
    "            #last_test_perf = test(model, 'cuda', test_loader, \n",
    "            #                      batch_size=batch_size, \n",
    "            #                      )\n",
    "            loss_track['avg_loss'] = np.mean(losses)\n",
    "            loss_track['last_test'] = last_test_perf\n",
    "            loss_track['epoch'] = epoch\n",
    "            loss_track['batch_idx'] = batch_idx\n",
    "            loss_track['train_perf']= np.sum(perfs)/((len(perfs))*batch_size)\n",
    "            with open(perf_file, 'a+') as fp:\n",
    "                csv_writer = DictWriter(fp, fieldnames=list(loss_track.keys()))\n",
    "                if fp.tell() == 0:\n",
    "                    csv_writer.writeheader()\n",
    "                csv_writer.writerow(loss_track)\n",
    "                fp.flush()\n",
    "            #if best_test_perf < last_test_perf:\n",
    "            #    torch.save(model.state_dict(), perf_file[:-4]+\".pt\")\n",
    "            #    best_test_perf = last_test_perf\n",
    "\n",
    "                \n",
    "def test(model, device, test_loader, batch_size=4):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device).view(data.shape[0],1,1,-1)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            out = model(data)\n",
    "            pred = out[:, -1].argmax(dim=-1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            count += 1\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T13:46:18.898666Z",
     "start_time": "2021-03-15T13:46:15.331245Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = 0.0\n",
    "sith_params1 = {\"in_features\":1, \n",
    "                \"tau_min\":1, \"tau_max\":30.0, \"buff_max\":50,\n",
    "                \"k\":125, 'dt':1,\n",
    "                \"ntau\":20, 'g':g,  \n",
    "                \"ttype\":ttype, \"batch_norm\":True,\n",
    "                \"hidden_size\":60, \"act_func\":nn.ReLU()\n",
    "               }\n",
    "sith_params2 = {\"in_features\":sith_params1['hidden_size'], \n",
    "                \"tau_min\":1, \"tau_max\":150.0, \"buff_max\":250,\n",
    "                \"k\":61, 'dt':1,\n",
    "                \"ntau\":20, 'g':g, \n",
    "                \"ttype\":ttype, \"batch_norm\":True,\n",
    "                \"hidden_size\":60, \"act_func\":nn.ReLU()\n",
    "                }\n",
    "sith_params3 = {\"in_features\":sith_params2['hidden_size'], \n",
    "                \"tau_min\":1, \"tau_max\":750.0, \"buff_max\":1500,\n",
    "                \"k\":35, 'dt':1,\n",
    "                \"ntau\":20, 'g':g, \n",
    "                \"ttype\":ttype, \"batch_norm\":True,\n",
    "                \"hidden_size\":60, \"act_func\":nn.ReLU()\n",
    "                }\n",
    "\n",
    "layer_params = [sith_params1, sith_params2, sith_params3]\n",
    "\n",
    "\n",
    "\n",
    "model = DeepSITH_Classifier(10,\n",
    "                           layer_params=layer_params, \n",
    "                           dropout=0.2).cuda()\n",
    "\n",
    "tot_weights = 0\n",
    "for p in model.parameters():\n",
    "    tot_weights += p.numel()\n",
    "print(\"Total Weights:\", tot_weights)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T07:38:12.591705Z",
     "start_time": "2021-03-15T13:46:45.607305Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "sched = StepLR(optimizer, step_size=int(epochs / 4), gamma=0.1)\n",
    "#sched = None\n",
    "perf_file = join('perf','smnist_deepsith_4layer_01.csv')\n",
    "test_perf = []\n",
    "progress_bar = tqdm(range(int(epochs)), bar_format='{l_bar}{bar:5}{r_bar}{bar:-5b}')\n",
    "best_test_perf = 0\n",
    "t_p = .0000\n",
    "for e in progress_bar:\n",
    "    train(model, ttype, train_loader, test_loader, optimizer, loss_func, batch_size=batch_size,\n",
    "          epoch=e, perf_file=perf_file,loss_buffer_size=64*32, \n",
    "          prog_bar=progress_bar, last_test_perf=t_p)\n",
    "    \n",
    "    t_p = test(model, 'cuda', test_loader, \n",
    "               batch_size=batch_size, \n",
    "               )\n",
    "    if t_p > best_test_perf:\n",
    "        best_test_perf = t_p\n",
    "        torch.save(model.state_dict(), perf_file[:-4]+\".pt\")\n",
    "        \n",
    "    test_perf.append({\"epoch\":e,\n",
    "                      'test':t_p})\n",
    "    \n",
    "    sched.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "nav_menu": {
    "height": "163px",
    "width": "250px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
